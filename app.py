import streamlit as st
import os
import pandas as pd
import numpy as np
import faiss
import time
from dotenv import load_dotenv
import google.generativeai as genai
from sentence_transformers import SentenceTransformer

# .env dosyasÄ±ndaki API anahtarÄ±nÄ± yÃ¼kle
load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

# Hata kontrolÃ¼
if not GOOGLE_API_KEY:
    st.error("GOOGLE_API_KEY bulunamadÄ±. LÃ¼tfen .env dosyanÄ±zÄ± kontrol edin.")
    st.stop()

genai.configure(api_key=GOOGLE_API_KEY)

# --- YardÄ±mcÄ± Fonksiyonlar (Colab'dan alÄ±ndÄ±) ---

def get_gemini_response(question, chat_history):
    """
    Gemini modelinden yanÄ±t alÄ±r.
    """
    model = genai.GenerativeModel('gemini-pro')
    chat = model.start_chat(history=chat_history)
    
    try:
        response = chat.send_message(question, stream=False)
        return response.text
    except Exception as e:
        st.error(f"Gemini API hatasÄ±: {e}")
        return "ÃœzgÃ¼nÃ¼m, bir hata oluÅŸtu."

def get_faiss_index(texts, model, index_path="faiss_index_v2"):
    """
    Metinlerden bir FAISS index'i oluÅŸturur veya yÃ¼kler.
    """
    if os.path.exists(index_path):
        try:
            # Varolan index'i yÃ¼kle
            index = faiss.read_index(index_path)
            print("Varolan FAISS index'i yÃ¼klendi.")
            return index
        except Exception as e:
            print(f"Index yÃ¼klenirken hata: {e}. Index yeniden oluÅŸturulacak.")

    # Yeni index oluÅŸtur
    print("Yeni FAISS index'i oluÅŸturuluyor...")
    try:
        embeddings = model.encode(texts, convert_to_tensor=False)
        
        # FAISS index'ini oluÅŸtur
        dimension = embeddings.shape[1]
        index = faiss.IndexFlatL2(dimension)  # L2 (Euclidean) mesafesi
        index.add(np.array(embeddings).astype('float32'))
        
        # Index'i diske kaydet
        faiss.write_index(index, index_path)
        print("Yeni index oluÅŸturuldu ve kaydedildi.")
        return index
    except Exception as e:
        st.error(f"FAISS index oluÅŸturulurken hata: {e}")
        return None

def get_context_from_faiss(index, query, model, k=5):
    """
    FAISS index'inden ilgili baÄŸlamÄ± alÄ±r.
    """
    try:
        query_embedding = model.encode([query], convert_to_tensor=False)
        distances, indices = index.search(np.array(query_embedding).astype('float32'), k)
        
        # 'texts' listesine ihtiyacÄ±mÄ±z olacak. Bu listeyi global'de veya
        # cache'lenmiÅŸ fonksiyondan almamÄ±z gerekiyor.
        # Bu fonksiyonun Ã§alÄ±ÅŸmasÄ± iÃ§in 'texts' listesine eriÅŸim varsayÄ±lÄ±yor.
        # Daha iyi bir yapÄ± iÃ§in 'texts' listesini de parametre olarak alabilir.
        # Åimdilik, 'texts'in bu fonksiyonun Ã§aÄŸrÄ±ldÄ±ÄŸÄ± yerde mevcut olduÄŸunu varsayÄ±yoruz.
        
        # 'texts' listesi 'load_resources' fonksiyonunda tanÄ±mlÄ±,
        # bu yÃ¼zden bu fonksiyonu 'load_resources' iÃ§inde kullanmak
        # veya 'texts'i de cache'lemek daha iyi olur.
        # Ancak Colab'daki yapÄ±ya sadÄ±k kalmak iÃ§in, 'texts' listesini
        # ana uygulamada yÃ¼klÃ¼yoruz ve bu fonksiyonu orada Ã§aÄŸÄ±rÄ±yoruz.
        
        # DÃ¼zeltme: 'texts' listesini de dÃ¶ndÃ¼relim.
        return indices[0] # Sadece index'leri dÃ¶ndÃ¼r, metinleri ana fonksiyonda al
    
    except Exception as e:
        st.error(f"FAISS aramasÄ± sÄ±rasÄ±nda hata: {e}")
        return []

def safe_text_extraction(row):
    """
    Veri Ã§erÃ§evesi satÄ±rÄ±ndan metin Ã§Ä±karÄ±r.
    """
    try:
        return f"Soru: {row['Soru']} Cevap: {row['Cevap']}"
    except TypeError:
        return "" # HatalÄ± veya eksik veri varsa boÅŸ dÃ¶ndÃ¼r

# --- Veri YÃ¼kleme ve Ã–nbelleÄŸe Alma ---

@st.cache_resource
def load_resources():
    """
    AÄŸÄ±r kaynaklarÄ± (model, veri, index) yÃ¼kler ve cache'ler.
    Bu fonksiyon uygulama baÅŸlarken sadece bir kez Ã§alÄ±ÅŸÄ±r.
    """
    st.info("Kaynaklar yÃ¼kleniyor (Bu iÅŸlem birkaÃ§ dakika sÃ¼rebilir)...")
    
    # 1. GÃ¶mme (Embedding) Modelini YÃ¼kle
    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
    
    # 2. VeritabanÄ±nÄ± YÃ¼kle
    try:
        df = pd.read_excel('combined_data_v2.xlsx')
    except FileNotFoundError:
        st.error("HATA: 'combined_data_v2.xlsx' dosyasÄ± proje klasÃ¶rÃ¼nde bulunamadÄ±.")
        st.stop()
        
    # 3. Metinleri HazÄ±rla
    df['text'] = df.apply(safe_text_extraction, axis=1)
    texts = df['text'].dropna().tolist()
    
    if not texts:
        st.error("VeritabanÄ±ndan metin okunamadÄ±.")
        st.stop()
        
    # 4. FAISS Index'ini YÃ¼kle veya OluÅŸtur
    faiss_index = get_faiss_index(texts, embedding_model)
    
    if faiss_index is None:
        st.error("FAISS index'i yÃ¼klenemedi veya oluÅŸturulamadÄ±.")
        st.stop()
        
    st.success("Kaynaklar baÅŸarÄ±yla yÃ¼klendi! Chatbot hazÄ±r.")
    
    # 'texts' listesini de dÃ¶ndÃ¼rÃ¼yoruz ki RAG aramasÄ±nda kullanabilelim
    return embedding_model, faiss_index, texts

# --- Streamlit UygulamasÄ± ---

st.set_page_config(page_title="EcoLife Chatbot", page_icon="ğŸŒ±")
st.title("ğŸŒ± EcoLife - Vegan & Ekolojik YaÅŸam AsistanÄ±")
st.caption("Akbank Generative-AI Bootcamp Projesi")

# KaynaklarÄ± yÃ¼kle (cache'den gelir)
try:
    embedding_model, faiss_index, texts = load_resources()
except Exception as e:
    st.error(f"Kaynaklar yÃ¼klenirken kritik bir hata oluÅŸtu: {e}")
    st.stop()


# Oturum durumunu (chat geÃ§miÅŸi) baÅŸlat
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
    st.session_state.chat_history.append({
        "role": "assistant",
        "content": "Merhaba! Ben EcoLife. VeganlÄ±k ve ekolojik yaÅŸam hakkÄ±nda sorularÄ±nÄ±zÄ± yanÄ±tlamak iÃ§in buradayÄ±m."
    })


# Chat geÃ§miÅŸini ekrana yazdÄ±r
for message in st.session_state.chat_history:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# KullanÄ±cÄ±dan yeni giriÅŸ al
if prompt := st.chat_input("VeganlÄ±k veya ekolojik yaÅŸam hakkÄ±nda bir soru sorun..."):
    
    # KullanÄ±cÄ± mesajÄ±nÄ± gÃ¶ster ve geÃ§miÅŸe ekle
    st.chat_message("user").markdown(prompt)
    st.session_state.chat_history.append({"role": "user", "content": prompt})

    # RAG: Ä°lgili baÄŸlamÄ± FAISS'den al
    context_indices = get_context_from_faiss(faiss_index, prompt, embedding_model, k=5)
    
    # Ä°ndex'lere gÃ¶re metinleri al
    context_texts = [texts[i] for i in context_indices]
    
    # Gemini iÃ§in birleÅŸtirilmiÅŸ prompt hazÄ±rla
    combined_prompt = f"""
    KullanÄ±cÄ± Sorusu: {prompt}

    Bilgi TabanÄ±ndan AlÄ±nan Ä°lgili BaÄŸlam (LÃ¼tfen cevabÄ±nÄ± bu baÄŸlama dayandÄ±r):
    {"---".join(context_texts)}

    LÃ¼tfen YALNIZCA saÄŸlanan baÄŸlamÄ± kullanarak kullanÄ±cÄ± sorusunu yanÄ±tla. EÄŸer cevap baÄŸlamda yoksa, 'Bu konuda bilgim bulunmuyor, ancak farklÄ± bir ÅŸekilde sorabilir misiniz?' de.
    """

    # Modeli Ã§aÄŸÄ±r ve yanÄ±tÄ± al
    with st.spinner("EcoLife dÃ¼ÅŸÃ¼nÃ¼yor..."):
        response_text = get_gemini_response(combined_prompt, st.session_state.chat_history)
    
    # YanÄ±tÄ± gÃ¶ster ve geÃ§miÅŸe ekle
    with st.chat_message("assistant"):
        st.markdown(response_text)
    st.session_state.chat_history.append({"role": "assistant", "content": response_text})

