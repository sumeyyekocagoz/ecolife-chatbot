import streamlit as st
import os
import pandas as pd
import numpy as np
import faiss
import time
from dotenv import load_dotenv
import google.generativeai as genai
from sentence_transformers import SentenceTransformer

# .env dosyasÄ±ndaki API anahtarÄ±nÄ± yÃ¼kle (Bu, lokal Ã§alÄ±ÅŸma iÃ§indir. 
# Streamlit Cloud'da secrets kullanÄ±lÄ±r)
load_dotenv()

# --- API AnahtarlarÄ±nÄ± YÃ¼kleme ve Kontrol Etme ---

# Google API anahtarÄ±nÄ± al (Ã–nce Streamlit Secrets'tan dene, sonra lokal .env'den)
GOOGLE_API_KEY = st.secrets.get("GOOGLE_API_KEY", os.getenv("GOOGLE_API_KEY"))

# Hugging Face token'Ä±nÄ± al (Sadece Streamlit Secrets'tan)
HF_TOKEN = st.secrets.get("HUGGING_FACE_HUB_TOKEN")

# Anahtar kontrolleri
if not GOOGLE_API_KEY:
    st.error("GOOGLE_API_KEY bulunamadÄ±. LÃ¼tfen Streamlit Secrets'Ä± kontrol edin.")
    st.stop()

if not HF_TOKEN:
    st.error("HUGGING_FACE_HUB_TOKEN bulunamadÄ±. LÃ¼tfen Streamlit Secrets'Ä± kontrol edin.")
    st.stop()

genai.configure(api_key=GOOGLE_API_KEY)

# --- YardÄ±mcÄ± Fonksiyonlar ---

def get_gemini_response(question, chat_history):
    # DÃœZELTME: En modern 'gemini-1.5-flash' modelini kullanÄ±yoruz.
    # requirements.txt'deki sÃ¼rÃ¼m gÃ¼ncellemesi bu modelin bulunmasÄ±nÄ± saÄŸlayacak.
    model = genai.GenerativeModel('gemini-1.5-flash') 
    
    chat = model.start_chat(history=chat_history) 
    
    try:
        response = chat.send_message(question, stream=False)
        return response.text
    except Exception as e:
        st.error(f"Gemini API hatasÄ±: {e}")
        return "ÃœzgÃ¼nÃ¼m, bir hata oluÅŸtu."

def get_faiss_index(texts, model, index_path="faiss_index_v2"):
    if os.path.exists(index_path):
        try:
            index = faiss.read_index(index_path)
            print("Varolan FAISS index'i yÃ¼klendi.")
            return index
        except Exception as e:
            print(f"Index yÃ¼klenirken hata: {e}. Index yeniden oluÅŸturulacak.")

    print("Yeni FAISS index'i oluÅŸturuluyor...")
    try:
        embeddings = model.encode(texts, convert_to_tensor=False)
        dimension = embeddings.shape[1]
        index = faiss.IndexFlatL2(dimension)
        index.add(np.array(embeddings).astype('float32'))
        faiss.write_index(index, index_path)
        print("Yeni index oluÅŸturuldu ve kaydedildi.")
        return index
    except Exception as e:
        st.error(f"FAISS index oluÅŸturulurken hata: {e}")
        return None

def get_context_from_faiss(index, query, model, k=5):
    try:
        query_embedding = model.encode([query], convert_to_tensor=False)
        distances, indices = index.search(np.array(query_embedding).astype('float32'), k)
        return indices[0] 
    except Exception as e:
        st.error(f"FAISS aramasÄ± sÄ±rasÄ±nda hata: {e}")
        return []

def safe_text_extraction(row):
    try:
        # JSONL dosyanÄ±zdaki anahtarlarÄ±n 'Soru' ve 'Cevap' olduÄŸunu varsayÄ±yorum.
        return f"Soru: {row['Soru']} Cevap: {row['Cevap']}"
    except KeyError:
        if 'text' in row:
            return row['text']
        return "" 
    except TypeError:
        return "" 

# --- Veri YÃ¼kleme ve Ã–nbelleÄŸe Alma ---

@st.cache_resource():
def load_resources():
    st.info("Kaynaklar yÃ¼kleniyor (Bu iÅŸlem birkaÃ§ dakika sÃ¼rebilir)...")
    
    try:
        embedding_model = SentenceTransformer(
            'all-MiniLM-L6-v2',
            token=HF_TOKEN
        )
    except Exception as e:
        st.error(f"SentenceTransformer yÃ¼klenirken hata oluÅŸtu. Hata: {e}")
        st.stop()
        
    DATA_FILE = 'llama.jsonl'
    try:
        df = pd.read_json(DATA_FILE, lines=True)
    except FileNotFoundError:
        st.error(f"HATA: '{DATA_FILE}' dosyasÄ± proje klasÃ¶rÃ¼nde bulunamadÄ±.")
        st.stop()
    except Exception as e:
        st.error(f"'{DATA_FILE}' dosyasÄ± okunurken hata: {e}")
        st.stop()
        
    df['text'] = df.apply(safe_text_extraction, axis=1)
    texts = df['text'].dropna().tolist()
    
    if not texts:
        st.error("VeritabanÄ±ndan metin okunamadÄ±.")
        st.stop()
        
    faiss_index = get_faiss_index(texts, embedding_model)
    
    if faiss_index is None:
        st.error("FAISS index'i yÃ¼klenemedi veya oluÅŸturulamadÄ±.")
        st.stop()
        
    st.success("Kaynaklar baÅŸarÄ±yla yÃ¼klendi! Chatbot hazÄ±r.")
    
    return embedding_model, faiss_index, texts

# --- Streamlit UygulamasÄ± ---

st.set_page_config(page_title="EcoLife Chatbot", page_icon="ğŸŒ±")
st.title("ğŸŒ± EcoLife - Vegan & Ekolojik YaÅŸam AsistanÄ±")
st.caption("Akbank Generative-AI Bootcamp Projesi")

try:
    embedding_model, faiss_index, texts = load_resources()
except Exception as e:
    st.error(f"Kaynaklar yÃ¼klenirken kritik bir hata oluÅŸtu: {e}")
    st.stop()

# Oturum durumunu (chat geÃ§miÅŸi) baÅŸlat
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
    st.session_state.chat_history.append({
        "role": "model", 
        "parts": [{"text": "Merhaba! Ben EcoLife. VeganlÄ±k ve ekolojik yaÅŸam hakkÄ±nda sorularÄ±nÄ±zÄ± yanÄ±tlamak iÃ§in buradayÄ±m."}] 
    })

# Chat geÃ§miÅŸini ekrana yazdÄ±r
for message in st.session_state.chat_history:
    display_role = "assistant" if message["role"] == "model" else message["role"]
    with st.cache_resource:
        st.markdown(message["parts"][0]["text"])

# KullanÄ±cÄ±dan yeni giriÅŸ al
if prompt := st.chat_input("VeganlÄ±k veya ekolojik yaÅŸam hakkÄ±nda bir soru sorun..."):
    
    st.chat_message("user").markdown(prompt)
    st.session_state.chat_history.append({"role": "user", "parts": [{"text": prompt}]})

    context_indices = get_context_from_faiss(faiss_index, prompt, embedding_model, k=5)
    context_texts = [texts[i] for i in context_indices]
    
    combined_prompt = f"""
    KullanÄ±cÄ± Sorusu: {prompt}
    Bilgi TabanÄ±ndan AlÄ±nan Ä°lgili BaÄŸlam (LÃ¼tfen cevabÄ±nÄ± bu baÄŸlama dayandÄ±r):
    {"---".join(context_texts)}
    LÃ¼tfen YALNIZCA saÄŸlanan baÄŸlamÄ± kullanarak kullanÄ±cÄ± sorusunu yanÄ±tla. EÄŸer cevap baÄŸlamda yoksa, 'Bu konuda bilgim bulunmuyor.' de.
    """

    with st.spinner("EcoLife dÃ¼ÅŸÃ¼nÃ¼yor..."):
        response_text = get_gemini_response(combined_prompt, st.session_state.chat_history)
    
    with st.chat_message("assistant"):
        st.markdown(response_text)
    st.session_state.chat_history.append({"role": "model", "parts": [{"text": response_text}]})


